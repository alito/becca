\chapter{Related Work}

NOTE: This chapter is incomplete as it stands. Consider its deficiencies and inaccuracies as an invitation to contribute to it.

Most machine learning techniques can be broadly classified as either supervised, unsupervised, or reinforcement learning. Supervised learning techniques use data, labeled with either a category or a value, to infer the category or value of unlabeled data types. Typically they learn on a human-labeled training data set, then process an unlabeled data set that is much larger. Facebook's face labeling system is a popular example of a supervised learner.

\section{Unsupervised Learning}
Unsupervised learning is primarily concerned with clustering data. Unlabeled data points are grouped based on a given distance metric or similarity measure. Centroids of the resulting clusters can be used to represent the various classes of observations. Some unsupervised learning algorithms learn these centroids directly. The centroids can be used as features by which to classify future observations. Common methods for unsupervised learning include principal components analysis (PCA), k-means, vector quantization, and perceptrons and other neural networks. (Latent elements in a multilayer perceptron represent cluster centroids.) Besides clustering, other terms for unsupervised learning can include compression, basis discovery, and dimensionality reduction.

Unsupervised learners may be create either a fixed number of clusters or a hierarchical tree in which the number of clusters is determined by selecting a cut point. There is a second sense in which an unsupervised learning method may be hierarchical. Some methods not only perform clustering on observations, but also on the clusters themselves. By iteratively creating clusters in this way, a hierarchy of clusters results. \textsc{Becca}'s feature extractor is hierarchical in this second sense.

\textsc{Becca}'s feature extractor is unique in that it is the only hierarchical unsupervised learning method that does not specify the number of clusters or the number of levels in the hierarchy. In deep neural networks, the number of layers and number of elements per layer can effect the performance of the feature extractor significantly.~\cite{montavon11} The ability to generate feature hierarchies automatically removes once source of human engineering when applying a feature extractor to novel problems, and increases \textsc{Becca}'s generality.

The problem of hierachical feature extraction is closely related to {\bf deep learning}.~\cite{arel10} Deep learning approaches seek to discover and exploit the underlying structure of a world by creating higher level, lower-dimensional representations of the system's input space. Deep learning algorithms include Convolutional Neural Networks (CNN)~\cite{lecun98}, Deep Belief Networks (DBN)~\cite{hinton06,fasel10b}, Hierarchical Temporal Memory (HTM)~\cite{hawkins11}, and the Deep SpatioTemporal Inference Network (DeSTIN)~\cite{arel09}. Deep learning algorithms such as these are alternative approaches, worthy of consideration for automatic concept acquisition, although they differ somewhat from \textsc{Becca}'s feature extractor. CNNs are designed to work with two-dimensional data, such as images, and they do not apply to arbitrarily structured data, as \textsc{Becca} does. By using several layers of Restricted Boltzmann Machines, DBNs are capable of generating sophisticated features that allow it to interpret novel inputs. However, they are typically applied to the supervised learning problem of discrimination, and require a substantial amount of labeled data in order to be adequately trained. Whether DBNs can be applied to the unsupervised learning problem of feature extraction is unclear. HTM has been described conceptually and in pseudocode, but no results have been published and the method has not been subjected to peer review. However, it is of interest in that it promises to create hierarchical feature sets that incorporate the ability to store and predict temporal sequences of activity, combining \textsc{Becca}'s feature extraction and model extraction functions into one element. DeSTIN incorporates both unsupervised and supervised learning methods and appears to be fully capable of hierarchical feature extraction. It has been published only recently; future papers describing its operation and performance will allow a more detailed comparison with \textsc{Becca}'s feature extractor.


\section{Reinforcement Learning}
Reinforcement learning (RL) is focused on the problem of choosing actions in order to maximize a reward. The canonical formulation of RL is given in~\cite{sutton98}. Because it is focused on action selection and because it requires no training information other than reward, RL is well suited to autonomous agents.

A reinforcement learner may be model-based or model-free. In the model-free case, the learner chooses actions based only on the value of its current state. In the model-based learners, some explicit representation of the agent's history is retained, allowing the learner to plan. In this way, a model is bootstrapped by the learner during its lifetime. \textsc{Becca}'s reinforcement learner is an example of model-based RL. If implemented appropriately, model-based RLs can adapt to changes in the reward function more quickly than model-free RLs, which must learn not only a new reward function, but values it has already associated with other states and actions. \textsc{Becca}'s RL retains its model information and learns only the changing reward function.

There are several well known methods for solving the reinforcement learning problem, that is, given state inputs and a reward at each timestep, choose actions that maximize the future reward. Some examples that have been applied in agents include Q-learning~\cite{watkins92}, the Dyna architecture~\cite{sutton91}, Associative Memory~\cite{levinson05}, and neural-network-based techniques including Brain-Based Devices~\cite{mckinstry06} and CMAC~\cite{albus75}. \textsc{Becca}'s reinforcement learner is another such algorithm. It is {\em on-line} and {\em model-based}, meaning that as it accumulates experience it creates and refines an internal model of itself and its environment. It differs from most previous methods in two ways. First, its internal model is not a first order Markov model. Instead, by using cause-effect transition pairs in which the cause is a compressed version of the agent's recent state history, it creates a compressed higher order Markov model. This potentially allows \textsc{Becca} to learn more sophisticated state dynamics and to record distinct sequences more naturally. Second, \textsc{Becca}'s reinforcement learning algorithm can handle a growing state space. This is necessary because it must work in tandem with \textsc{Becca}'s feature extractor, which continues to identify new features throughout the life of the agent.

\textsc{Becca}'s feature extractor and reinforcement learner are {\em incremental}, meaning that they can be efficiently updated with single observations, and {\em on-line}, meaning that this update can take place quickly enough to happen in real time during the agent's operation.

One challenging RL problem is the general Partially Observable Markov Decision Process (POMDP). Exact solutions to general POMDPs are computationally intractable, but some approximate solutions have been put forward. As challenging as they are, POMDPs are a subset of the Natural World Interaction problem. POMDPs are assumed to contain a stationary Markov Decision Process, whereas NWI problems may be functions of time.

\section{Unsupervised Feature Learning with Reinforcement Learning}
There are several recent examples of RL combined with unsupervised feature learning. A brief description of each describes the feature learning algorithm and the RL algorithm used and a list of the tasks to which it has been applied. A list of key differences between each approach and \textsc{Becca} is included as well.

\subsection{VQ-SNES} 
A team based at IDSIA use online vector quantization (VQ) as an unsupervised learner and Separable Natural Evolution Strategies (SNES), an evolutionary recurrent neural network RL, to learn a vision-based variation of the Mountain Car task.~\cite{cuccu11} The RL was rewarded not only for good performance on the task, but also for high reconstruction error. This drove it to search for novel visual inputs, as well as to complete the task quickly.

Some differences between \textsc{Becca} and VQ-SNES:
\begin{itemize}
\item VQ-SNES training occurs in batches, corresponding to evolutionary generations, rather than on-line.
\item VQ fails in the presence of background noise. \textsc{Becca}'s feature extractor is designed to be robust to background noise.
\item VQ does not create a hierarchical feature set.
\item SNES is model-free.
\end{itemize}

\subsection{DBN-NFQ} 
A University of Arizona team is using Deep Belief Networks (DBN) to initialize Neural-Fitted Q-learning (NFQ).~\cite{abtahi11} They demonstrate their work on Mountain Car and on a two-dimensional grid world with obstacles of their creation, Puddle World. NFQ uses a multilayer perceptron to perform function approximation when learning the value function, $Q$. DBNs create a multilayer Restricted Boltzmann Machine directly from observing data. The U of A team initialized a DBN in an unsupervised manner, and then used that as the initial multilayer perceptron for learning the value function in NFQ.

Some differences between \textsc{Becca} and DBN-NFQ:

\begin{itemize}
\item In order to produce good results, the data used to train DBNs is often selected by the programmer, incorporating significant knowledge of the problem domain. Care must be taken to avoid over- under-sampling.
\item Training takes place in a distinct epoch, separately from behavior learning and performance. As a result, feature learning ceases and the number of features used to represent the world becomes fixed. This is not the case in \textsc{Becca}.
\item NFQ learns a value function. If the underlying reward function changes, the entire value function must be re-learned. \textsc{Becca} avoids this by learning a state transition model separately from a reward function.
\end{itemize}

\subsection{SFA-NC} 
An inter-institute team from Graz University and Humbolt University combined hierarchical Slow Feature Analysis (SFA) and two neural circuit (NC) implementations of RL methods.~\cite{legenstein10} The most interesting aspect of the work was the application of hierarchical SFA as an unsupervised method. SFA searches for slowly varying aspects of the input data and extracts those as features. In hierarchical SFA, a network of nodes process data in multiple levels, with the features extracted in one level serving as the inputs to the next. On the RL side, neural circuit implementations of both Q-learning and a policy gradient method were used. The authors demonstrated the method on a Morris water maze-type task and a vision-based two-dimensional navigation task involving a ball, a cross, and a fish.

Some differences between \textsc{Becca} and SFA-NC:

\begin{itemize}
\item SFA, as implemented by the authors, has a fixed number of nodes and a fixed number of hierarchy layers. The network structure is fixed at the time of creation. It's design reflects a great deal of knowledge about the nature of the sensory information being processed. This is in contrast to \textsc{Becca}'s feature extractor, whose grouping of inputs and hierarchical structure is driven by the data observed.
\item SFA undergoes a training period separate from the performance period. Training inputs are user-crafted and incorporate knowledge of the nature of the sensors.
\item The NC methods employed cannot handle a changing number of inputs.
\end{itemize}

\subsection{ISQL-MDQL} 
Discretization is the process of converting continuous state spaces into discrete ones, or of converting discrete state spaces in to more coarsely discretized spaces. It may be considered a degenerate case of feature extraction, although it is weaker than other feature extraction methods in that it cannot reduce the dimensionality of the learning problem. For certain problems, appropriate discretization can greatly decrease computational requirements. However, the problems on which this is most commonly demonstrated involve planar navigation, an task set that can easily be expressed using a low-dimensional state space.

A team at Universidad Carlos III de Madrid presented a methods of automatic state space discretization, iterative smooth Q-learning (ISQL), paired with a reinforcement learning algorithm called multiple discretization Q-learning (MDQL). ISQL is interesting as a discretization method in that it learns a discretization based on the performance of a preliminary RL algorithm. It can sole RL problems by itself, but when its discretization is output to for use by MDQL, the learning rate increases. Together, ISQL and MDQL are called Two Steps RL.~\cite{fernandez08} Two Steps RL has been demonstrated on a continuous grid world problem resembling an office building, on Mountain Car, and on Acrobot.

Some differences between \textsc{Becca} and ISQL-MDQL:

\begin{itemize}
\item Both ISQL and MDQL rely on variants of Q-learning, a model-free learning method that relies on value approximation to learn large state spaces.
\item ISQL requires a separate training period before its state space discretization can be handed over to MDQL.
\item Due to the face that ISQL performs discretization only (a non-hierarchical type feature extraction) \textsc{Becca} has potentially a much richer representational capacity.
\item It is unclear how well ISQL-MDQL generalizes to higher-dimensional data. It has only been demonstrated on low-dimensional test problems.
\end{itemize}


\subsection{MLP-FQI} 
A University of Freiberg team paired a multi-layer perceptron (MLP) with Fitted Q-Iteration (FQI).~\cite{lange10} MLPs are classic neural networks that have been shown to perform well in generating low-dimensional representations (commpressions) of high-dimensional data. FQI is a batch RL method that calculates a Q-function (state-action value function) based on a collection of state-action-reward-state transitions. In their implementation, the authors maintain a history of all such transitions so that MLP can periodically re-compute its high-level features and FQI can generate an ever-better-informed Q-function. MLP-FQI was demonstrated in a discrete gridworld task using real digital images.

Some differences between \textsc{Becca} and MLP-FQI:

\begin{itemize}
\item Training of the MLP and FQI both are in batch mode. They don't incrementally update in an inexpensive manner.
\item MLP have a fixed number of layers and elements that is set at their creation.
\end{itemize}

\subsection{sRAAM-SARSA} 
The IDSIA team presents another unsupervised-reinforcement learning algorithm pair,~\cite{gisslen11} this one consisting of Sequential Recurrent Auto-Associative Memory (sRAAM) and SARSA($\lambda$), described in~\cite{sutton98}. sRAAM is a recurrent neural network that functions as an autoencoder capable of forming features that have not only spatial, but also temporal extent. It not only represents patterns in its inputs, but also patterns in how they change over time. It has been demonstrated solving high-dimensional, point-of-view vision-based maze navigation problems and problems that require learning long time delays. The algorithm incorporates a simple discretization (vector quantization) step to convert the output from sRAAM from a continuous to a discrete state space, so that SARSA, which is a tabular (table-based) RL algorithm could be used with it. The authors mention a lack of stability in the algorithm in the last paragraph of the paper, but do not elaborate.

Some differences between \textsc{Becca} and sRAAM-SARSA:

\begin{itemize}
\item sRAAM is described as a Sequential Constant-Size Compressor. It has a fixed number of features, and as implemented, the vector quantization step creates a fixed number of feature space regions.
\item sRAAM and SARSA($\lambda$) are batch algorithms. They do not gracefully update after each observation. They are bath-updated in alternating chunks. This precludes one-shot learning.
\end{itemize}


\section{Feature Extraction with Robots}

Although not implemented with RL algorithms, there are several feature extraction algorithms that have been implemented on robots, physically embodied systems with sensors that respond to changes in the physical world.

\subsection{HSSH} 
Automatic mapping of an environment can support model-based RL. It is a form of feature extraction. The Hybrid Spatial Semantic Hierarchy~\cite{pierce97,beeson10} work, performed at the University of Texas at Austin, is one such mapping methodology. It stitches local sensor snapshots into a global map topology. It is particulary notable for the fact that it makes few assumptions about its sensors and environment. This allows it to build a model of its own sensor arrangement from scratch, then use that model to take low-level sensor snapshots of its environment. It also incorporates the learned interaction between its motor behaviors changes in sensory information to make local maps of its environments. As published, however, it is not a complete FC-RL solution in that it has no reward signal it is trying to maximize. However, such a signal could very conceivably be implemented on top of it.

\subsection{DTW} 
A team at the University of Massachusetts at Amherst used Dynamic Time Warping (DTW) to aid in clustering temporal snippets of data from a mobile robot to create features that proved useful in navigation. The prototypes calculated from these clusters were surprisingly similar to features identified by human annotators. Clustering time series is not new, but the distance measure used was novel. By warping the sensor returns in time to align them, the most important aspects of the sensor information was preserved by making it invariant to variations in the speed with which a maneuver was executed.


\section{Reinforcement Learning with Robots}

Reinforcement learning is appropriate for learning in robots because it has an active component. An RL agent chooses actions, and so can direct its experience and indirectly influence its learning. This can, for instance, help solve the problem of grounding (providing a semantic interpretation for) ungrounded sensor inputs.~\cite{choe07}

A team centered at the {\em University of Alberta} has implemented {\em Horde}~\cite{sutton11}, an RL algorithm, on the Critterbot, a comma-shaped three degree of freedom mobile robot with a rich sensor suite. Horde is actually a collection of many RL demons, each performing GQ($\lambda$)~
\cite{maei10}, an off-policy gradient-descent temporal difference algorithm. They create features using tile coding. Each demon in Horde captures some aspect of semantic information about the agent's interaction with its environment. As a result, Horde is capable of representing very general classes of sensorimotor information. One apparent shortcoming of Horde is that the parameters of the demons must still be set in some way. It is unclear if this can be done in an automated manner or whether significant engineering and domain knowledge is required. A second shortcoming is that, rich though it may be, the knowledge that Horde can capture is limited by its demons. If no demons are created after the initiation of the system, its knowledge will be limited.

%Include review of Q-learning and robots.

%\subsubsection{Feature Creation and Reinforcement Learning with Robots}
%As far as I can tell, \textsc{Becca} is the only work in this space.
